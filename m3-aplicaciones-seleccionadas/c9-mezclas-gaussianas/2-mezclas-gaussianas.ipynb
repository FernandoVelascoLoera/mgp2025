{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de mezclas Gaussianas\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/71/Gaussian-mixture-example.svg\" width=\"500px\" height=\"300px\" />\n",
    "\n",
    "> El modelo de mezclas Gaussianas es un modelo de soft-clustering que nos permite modelar con precisión arbitraria un conjunto de datos continuos. Este poderoso algoritmo siempre conviene tenerlo en el arsenal para problemas de segmentación.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Comprender el modelo de mezclas Gaussianas.\n",
    "\n",
    "> **Referencias:**\n",
    "> - Bayesian Methods for Machine Learning course, HSE University, Coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos, el K-Means (y en general los algoritmos de hard clustering) tienen varios detalles:\n",
    "\n",
    "- No es claro cómo elegir el número de clusters.\n",
    "- Hay puntos que podrían estar en una frontera entre dos o más clusters, y el hard clustering no nos permite tener incertidumbre en la pertenencia.\n",
    "\n",
    "Para lidiar con estos problemas, podemos plantear un modelo probablístico de nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, conocemos algunas distribuciones. Entre ellas la Gaussiana, para la cual ya sabemos como estimar sus parámetros.\n",
    "\n",
    "¿Qué pasa si intentamos ajustar una **distribución Gaussiana** a los datos? Es decir, si modelamos los datos con\n",
    "\n",
    "$$\n",
    "p(x|\\theta) = \\mathcal{N}(x|\\mu, \\Sigma), \\qquad \\theta=\\{\\mu, \\Sigma\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos función para generar datos\n",
    "\n",
    "# Importamos pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos algunos ejemplos de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico de puntos (sin etiqueta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.stats.multivariate_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos parámetros\n",
    "\n",
    "# Definimos VA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "\n",
    "# Gaussiana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verosimilitud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-verosimilitud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, este modelo no parece corresponder con nuestros datos. La región de máxima probabilidad (media) cae en un punto medio entre los clusters, y allí no hay muchos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué pasa si usamos varias Gaussianas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos parámetros\n",
    "\n",
    "# Definimos VA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "\n",
    "\n",
    "# Gaussiana 1\n",
    "\n",
    "\n",
    "# Gaussiana 2\n",
    "\n",
    "\n",
    "# Gaussiana 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡Mucho mejor!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada Gaussiana explica un cluster de puntos, y el modelo general sería una suma ponderada de estas densidades Gaussianas:\n",
    "\n",
    "$$\n",
    "p(x | \\theta) = \\sum_{c=1}^{3} \\pi_c \\mathcal{N}(x | \\mu_c, \\Sigma_c), \\qquad \\theta = \\{\\pi_1, \\pi_2, \\pi_3, \\mu_1, \\mu_2, \\mu_3, \\Sigma_1, \\Sigma_2, \\Sigma_3\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Y esto cómo lo interpretamos?**\n",
    "\n",
    "Bueno, pues si logramos encontrar los parámetros $\\pi_1, \\pi_2, \\pi_3, \\mu_1, \\mu_2, \\mu_3, \\Sigma_1, \\Sigma_2, \\Sigma_3$ para este conjunto de datos, habremos resuelto el problema de (soft) clustering, ya que encontraremos para cada punto la probabilidad de que venga de cada una de las Gaussianas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué ventajas tenemos?**\n",
    "\n",
    "Como ventaja respecto a usar una sola Gaussiana, hemos añadido flexibilidad a nuestro modelo. Es decir, con esta estuctura podemos representar conjuntos de datos complejos.\n",
    "\n",
    "> En efecto, podemos aproximar casi cualquier distribución continua con una mezcla de Gaussianas con precisión arbitraria, dado que incluyamos un número suficiente de Gaussianas en la mezcla.\n",
    "\n",
    "**¿A qué costo?**\n",
    "\n",
    "La cantidad de parámetros que debemos estimar se multiplica por la cantidad de Gaussianas en la mezcla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo encontramos (entrenamos) los parámetros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos maximizar la función de verosimilitud (suposición de independencia):\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} p(X | \\theta) = \\prod_{i=1}^N p(x_i | \\theta) = \\prod_{i=1}^N \\sum_{c=1}^{3} \\pi_c \\mathcal{N}(x_i | \\mu_c, \\Sigma_c)\n",
    "$$\n",
    "\n",
    "sujeto a:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{c=1}^3 \\pi_c & = 1 \\\\\n",
    "\\pi_c & \\geq 0 \\quad \\text{for } c=1,2,3\\\\\n",
    "\\Sigma_c & \\succ 0 \\quad \\text{for } c=1,2,3\n",
    "\\end{align}\n",
    "\n",
    "Es decir, las matrices de covarianza deben ser definidas positivas (¿por qué?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complejidades numéricas:**\n",
    "\n",
    "Este problema de optimización puede ser resuelto numéricamente con un algoritmo como el gradiente descendiente. Sin embargo,\n",
    "\n",
    "1. La restricción sobre la matriz de covarianzas hace el problema de optimización muy complejo de resolver numéricamente hablando.\n",
    "\n",
    "   Una simplificación para poder trabajar con esta restricción es suponer que las matrices de covarianza son diagonales:\n",
    "\n",
    "$$\n",
    "\\Sigma_c = \\text{diag}(\\sigma_{c1}, \\sigma_{c2}, \\dots, \\sigma_{cn}),\n",
    "$$\n",
    "\n",
    "2. La suma dentro del producto también hace bastante complejo el cálculo de los gradientes. Comúnmente, para evitar el producto se toma logaritmo de la verosimilitud:\n",
    "\n",
    "   $$\n",
    "   \\log p(X | \\theta) = \\log \\left(\\prod_{i=1}^N p(x_i | \\theta) \\right)= \\sum_{i=1}^N \\log\\left(\\sum_{c=1}^{3} \\pi_c \\mathcal{N}(x_i | \\mu_c, \\Sigma_c)\\right)\n",
    "   $$\n",
    "\n",
    "   y con esto, podemos observar que permanece una suma ponderada dentro del logaritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Y entonces?\n",
    "\n",
    "Afortunadamente, existe un algoritmo alternativo con base probabilística llamado **algoritmo de maximización de la esperanza**, el cual no solo es útil para el problema de mezclas Gaussianas, sino para entrenar cualquier modelo con **variables latentes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Variables latentes?**\n",
    "\n",
    "Recordamos que propusimos el siguiente modelo:\n",
    "\n",
    "$$\n",
    "p(x | \\theta) = \\sum_{c=1}^{3} \\pi_c \\mathcal{N}(x | \\mu_c, \\Sigma_c), \\qquad \\theta = \\{\\pi_1, \\pi_2, \\pi_3, \\mu_1, \\mu_2, \\mu_3, \\Sigma_1, \\Sigma_2, \\Sigma_3\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo en realidad, lo podemos pensar como un modelo con una variable latente $t$ que determina a cuál Gaussiana pertenece cada punto:\n",
    "\n",
    "![gmm](figures/gmm.png)\n",
    "\n",
    "Entonces, razonablemente podemos atribuirle a $t$ tres posibles valores (1, 2, y 3), que nos dicen de qué Gaussiana viene el punto. Recordamos que $t$ es una variable latente, nunca la observamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, razonando probabilísticamente, después de entrenar nuestra mezcla Gaussiana, podríamos preguntarle al modelo, ¿Cuál es el valor más probable de $t$ dado el punto $x$? --> **Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este modelo, podemos asignar las siguientes probabilidades:\n",
    "\n",
    "- Previa:\n",
    "  \n",
    "  $$\n",
    "  p(t=c | \\theta) = \\pi_c\n",
    "  $$\n",
    "  \n",
    "- Verosimilitud:\n",
    "  \n",
    "  $$\n",
    "  p(x | t=c, \\theta) = \\mathcal{N}(x | \\mu_c, \\Sigma_c)\n",
    "  $$\n",
    " \n",
    "Razonable, ¿no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con lo anterior,\n",
    "\n",
    "$$\n",
    "p(x | \\theta) = \\sum_{c=1}^3 p(x, t=c | \\theta) = \\sum_{c=1}^3 \\underbrace{p(x | t=c, \\theta)}_{\\mathcal{N}(x | \\mu_c, \\Sigma_c)} \\underbrace{p(t=c | \\theta)}_{\\pi_c},\n",
    "$$\n",
    "\n",
    "justo como el modelo intuitivo que habíamos propuesto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algoritmo de maximización de la esperanza para mezclas Gaussianas - Intuición\n",
    "\n",
    "Supongamos que tenemos los siguientes puntos de tamaños de playeras, y queremos definir cuáles son talla chica y cuales son talla grande:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar shirts_size_data.generate_shirts_data\n",
    "\n",
    "# Importar scipy.stats.norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos y vemos ejemplos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo estimamos los parámetros de nuestro modelo de variable latente?\n",
    "\n",
    "Analicemos varios casos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Si de entrada supiéramos cuáles playeras son chicas y cuáles grandes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos camisas chicas\n",
    "\n",
    "# Datos camisas grandes\n",
    "\n",
    "# Ordenamos x para propósitos de gráficos (size)\n",
    "\n",
    "# Estimamos parámetros de la distribución normal para cada grupo\n",
    "\n",
    "# Graficamos las pdfs de las normales encontradas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x | t=1, \\theta) = \\mathcal{N}(x | \\mu_1, \\sigma_1)\n",
    "$$\n",
    "\n",
    "Con lo cual:\n",
    "\n",
    "$$\n",
    "\\mu_1 = \\frac{\\sum_{i: t_i = 1} x_i}{\\sum_{i: t_i = 1} 1}, \\qquad \\sigma_1^2 = \\frac{\\sum_{i: t_i = 1} (x_i - \\mu_1)^2}{\\sum_{i: t_i = 1} 1},\n",
    "$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\n",
    "\\mu_2 = \\frac{\\sum_{i: t_i = 2} x_i}{\\sum_{i: t_i = 2} 1}, \\qquad \\sigma_2^2 = \\frac{\\sum_{i: t_i = 2} (x_i - \\mu_1)^2}{\\sum_{i: t_2 = 1} 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Como sablemos, en el algoritmo de mezclas Gaussianas nunca sabremos si un punto pertenece a cierto cluster o no, sino que conoceremos las probabilidades de que pertenezca a cada cluster.\n",
    "\n",
    "   De modo que si conocemos la posterior $p(t | x, \\theta)$, entonces ponderamos lo anterior por esta probabilidad:\n",
    "   \n",
    "   $$\n",
    "\\mu_1 = \\frac{\\sum_{i} p(t_i=1 | x_i, \\theta)x_i}{\\sum_{i} p(t_i=1 | x_i, \\theta)}, \\qquad \\sigma_1^2 = \\frac{\\sum_{i} p(t_i=1 | x_i, \\theta) (x_i - \\mu_1)^2}{\\sum_{i} p(t_i=1 | x_i, \\theta)}.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Y cómo conocemos la posterior $p(t | x, \\theta)$?\n",
    "\n",
    "   Bueno, pues si conocemos los parámetros, es bastante fácil:\n",
    "   \n",
    "   $$\n",
    "   p(t=c | x, \\theta) = \\frac{p(x | t=c, \\theta) p(t=c | \\theta)}{Z} = \\frac{\\pi_c \\mathcal{N}(x | \\mu_c, \\sigma_c)}{Z}.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un razonamiento circular (un problema del tipo, ¿Qué fue primero?, ¿El huevo?, ¿O la gallina?). \n",
    "\n",
    "**¿Cómo lo resolvemos? Iterando...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmo de maximización de la esperanza:**\n",
    "\n",
    "1. Inicializamos los parámetros de cada Gaussiana aleatoriamente.\n",
    "\n",
    "2. Repetir hasta la convergencia:\n",
    "   \n",
    "   - Calcular para cada punto la probabilidad posterior $p(t_i=c | x_i, \\theta)$.\n",
    "   - Actualizar los parámetros de las Gaussianas con las probabildades calculadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Volviendo al problema original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos función para generar datos\n",
    "\n",
    "# Importamos sklearn.model_selection.train_test_split\n",
    "\n",
    "# Importamos sklearn.mixture.GaussianMixture\n",
    "\n",
    "# Importamos pyplot\n",
    "\n",
    "# Importamos numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help - GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de mezclas Gaussianas\n",
    "\n",
    "# Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros óptimos: means_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros óptimos: covariances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros óptimos: weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussianas ajustadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción de probabilidades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "\n",
    "\n",
    "# Gaussiana 1\n",
    "\n",
    "\n",
    "# Gaussiana 2\n",
    "\n",
    "\n",
    "# Gaussiana 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_gmm(X, mu, sigma, pi):\n",
    "    \"\"\"\n",
    "    Log-likelihood of the data wrt Gaussian Mixture Model.\n",
    "    :param data: Data.\n",
    "    :param mu: Means of the components of the GMM.\n",
    "    :param sigma: Covariances of the components of the GMM.\n",
    "    :param pi: Weights of the components of the GMM.\n",
    "    :return: Log-likelihood of the data wrt GMM.\n",
    "    \"\"\"\n",
    "    # Number of clusters\n",
    "    k = mu.shape[0]\n",
    "    # Number of points\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Individual likelihood of each point to each normal\n",
    "    ind_likelihood = np.zeros((N, k))\n",
    "    for j in range(k):\n",
    "        ind_likelihood[:, j] = multivariate_normal.pdf(X, mean=mu[j, :], cov=sigma[j, :, :])\n",
    "    \n",
    "    # Log likelihood\n",
    "    log_likelihood = np.log(ind_likelihood.dot(pi)).mean()\n",
    "        \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train y test\n",
    "\n",
    "log_likelihood_train = []\n",
    "log_likelihood_test = []\n",
    "for k in range(2, 11):\n",
    "    # Instanciamos el algoritmo\n",
    "    \n",
    "    # Entrenamos\n",
    "    \n",
    "    # Métrica con datos de entrenamiento\n",
    "    \n",
    "    \n",
    "    # Métrica con datos de prueba\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráifico de log-verosimilitud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
